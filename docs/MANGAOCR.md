# manga-ocr Technical Analysis

**Complete technical breakdown of the manga-ocr library**

Version: 0.1.14
Repository: https://github.com/kha-white/manga-ocr
Model: kha-white/manga-ocr-base (Hugging Face)

---

## Table of Contents

1. [Overview](#overview)
2. [Dependencies Analysis](#dependencies-analysis)
3. [Code Structure](#code-structure)
4. [Architecture & Data Flow](#architecture--data-flow)
5. [Model Files Breakdown](#model-files-breakdown)
6. [Docker Image Size Analysis](#docker-image-size-analysis)
7. [Optimization Recommendations](#optimization-recommendations)

---

## Overview

manga-ocr is a lightweight Python wrapper (~217 lines of code) around Hugging Face's transformers library, specifically designed for Japanese manga OCR. It uses a pre-trained Vision Transformer (ViT) encoder-decoder model.

**Key Stats:**

- Total code: 217 lines (5 files)
- Model size: 424MB (neural weights) + 100KB (configs)
- RAM usage: ~900MB-1GB during inference
- Performance: ~0.3-1s per image (GPU), ~1-3s (CPU)

---

## Dependencies Analysis

### Core Dependencies (pyproject.toml)

| Dependency             | Size              | Purpose                                            | Can Remove?        |
| ---------------------- | ----------------- | -------------------------------------------------- | ------------------ |
| `fire`                 | ~25KB             | CLI framework - auto-generates CLI from functions  | ‚ùå                 |
| `fugashi`              | ~1MB              | Japanese morphological analyzer for tokenization   | ‚ùå                 |
| `jaconv`               | ~50KB             | Japanese text conversion (half-width ‚Üî full-width) | ‚ùå                 |
| `loguru`               | ~100KB            | Modern logging library with colored output         | ‚ö†Ô∏è                 |
| `numpy`                | ~15-20MB          | Array operations (required by PyTorch)             | ‚ùå                 |
| `Pillow>=10.0.0`       | ~2-3MB            | Image loading and processing                       | ‚ùå                 |
| `pyperclip`            | ~10KB             | Clipboard access (cross-platform)                  | ‚ö†Ô∏è                 |
| **`torch>=1.0`**       | **~3-4GB (CUDA)** | Deep learning framework **[MAIN SIZE CULPRIT]**    | ‚úÖ Use CPU version |
| `transformers>=4.25.0` | ~500MB-1GB        | Hugging Face transformers (ViT + BERT)             | ‚ùå                 |
| `unidic_lite`          | ~50MB             | Japanese morphological dictionary                  | ‚ùå                 |

### Dependency Details

#### 1. `fire` (CLI Framework)

```python
# Used in: __main__.py, run.py
import fire
fire.Fire(run)  # Converts run() function into CLI
```

**Purpose:** Auto-generate CLI with argument parsing
**Example:** `manga_ocr --read-from clipboard --write-to output.txt`

#### 2. `fugashi` (Japanese Tokenizer)

**Purpose:** Japanese morphological analyzer for word segmentation
**Works with:** `unidic_lite` dictionary
**Used by:** transformers/tokenizer internally

#### 3. `jaconv` (Japanese Text Conversion)

```python
# Used in: ocr.py line 62
text = jaconv.h2z(text, ascii=True, digit=True)
```

**Purpose:** Normalize characters (half-width ‚Üí full-width)
**Example:** `ABC123` ‚Üí `Ôº°Ôº¢Ôº£ÔºëÔºíÔºì`

#### 4. `loguru` (Logging)

```python
# Used throughout: ocr.py, run.py
from loguru import logger
logger.info(f"Loading OCR model from {pretrained_model_name_or_path}")
logger.info(f"Text recognized in {t1 - t0:0.03f} s: {text}")
```

**Purpose:** Better logging than standard `logging` module
**Features:** Colored output, automatic formatting, timing info

#### 5. `numpy` (Numerical Arrays)

```python
# Used in: run.py line 19-22
def are_images_identical(img1, img2):
    img1 = np.array(img1)  # Convert PIL Image to numpy array
    img2 = np.array(img2)
    return (img1.shape == img2.shape) and (img1 == img2).all()
```

**Purpose:** Fast array operations for image comparison and tensor operations

#### 6. `Pillow` (Image Processing)

```python
# Used extensively: ocr.py, run.py
from PIL import Image, ImageGrab

img = Image.open(img_or_path)
img = img.convert("L").convert("RGB")  # Grayscale ‚Üí RGB
img = ImageGrab.grabclipboard()  # Read from clipboard
```

**Purpose:** Load images, format conversion, clipboard access

#### 7. `pyperclip` (Clipboard)

```python
# Used in: run.py
import pyperclip

pyperclip.copy(text)  # Copy OCR result to clipboard
pyperclip.set_clipboard("wl-clipboard")  # Wayland support
```

**Purpose:** Cross-platform clipboard access (Windows, Mac, Linux/Wayland)

#### 8. `torch` (PyTorch) üî¥ **CRITICAL**

```python
# Used in: ocr.py
import torch

# GPU/CPU detection
if not force_cpu and torch.cuda.is_available():
    self.model.cuda()  # NVIDIA GPU
elif not force_cpu and torch.backends.mps.is_available():
    self.model.to("mps")  # Apple Silicon
else:
    # CPU mode
```

**Purpose:** Deep learning framework for running Vision Transformer
**Size Issue:** Defaults to CUDA version (~3-4GB) instead of CPU (~900MB)

#### 9. `transformers` (Hugging Face)

```python
# Used in: ocr.py
from transformers import ViTImageProcessor, AutoTokenizer, VisionEncoderDecoderModel

self.processor = ViTImageProcessor.from_pretrained("kha-white/manga-ocr-base")
self.tokenizer = AutoTokenizer.from_pretrained("kha-white/manga-ocr-base")
self.model = MangaOcrModel.from_pretrained("kha-white/manga-ocr-base")
```

**Purpose:** Provides Vision Transformer architecture and pre-trained weights
**Components:** ViT encoder + BERT decoder

#### 10. `unidic_lite` (Japanese Dictionary)

**Purpose:** Lightweight Japanese morphological dictionary
**Used by:** `fugashi` for tokenization
**Size:** ~50MB dictionary data

---

## Code Structure

### File Organization

```
manga_ocr/
‚îú‚îÄ‚îÄ __init__.py          (2 lines)   - Package exports
‚îú‚îÄ‚îÄ __main__.py         (11 lines)   - CLI entry point
‚îú‚îÄ‚îÄ ocr.py              (64 lines)   - Core OCR logic ‚≠ê
‚îú‚îÄ‚îÄ run.py             (139 lines)   - Background watcher
‚îú‚îÄ‚îÄ _version.py          (1 line)    - Version string (0.1.14)
‚îî‚îÄ‚îÄ assets/
    ‚îî‚îÄ‚îÄ example.jpg     (56KB)       - Test image for warmup
```

**Total:** 217 lines of Python code

### File Breakdown

#### 1. `ocr.py` (64 lines) - The Brain üß†

**Main Classes:**

```python
class MangaOcrModel(VisionEncoderDecoderModel, GenerationMixin):
    pass  # Extends Hugging Face model

class MangaOcr:
    def __init__(self, pretrained_model_name_or_path="kha-white/manga-ocr-base",
                 force_cpu=False):
        # Load pre-trained components
        self.processor = ViTImageProcessor.from_pretrained(...)
        self.tokenizer = AutoTokenizer.from_pretrained(...)
        self.model = MangaOcrModel.from_pretrained(...)

        # Auto-detect GPU/CPU
        if not force_cpu and torch.cuda.is_available():
            self.model.cuda()
        elif not force_cpu and torch.backends.mps.is_available():
            self.model.to("mps")

        # Warmup run (makes first real OCR faster)
        self(example_path)

    def __call__(self, img_or_path):
        # Load and convert image
        img = Image.open(img_or_path).convert("L").convert("RGB")

        # Preprocess ‚Üí Generate ‚Üí Decode ‚Üí Post-process
        x = self._preprocess(img)
        x = self.model.generate(x[None].to(self.model.device), max_length=300)
        x = self.tokenizer.decode(x[0].cpu(), skip_special_tokens=True)
        return post_process(x)
```

**Post-processing:**

```python
def post_process(text):
    text = "".join(text.split())              # Remove whitespace
    text = text.replace("‚Ä¶", "...")            # Normalize ellipsis
    text = re.sub("[„Éª.]{2,}", ...)           # Multiple dots
    text = jaconv.h2z(text, ascii=True)       # Half‚ÜíFull width
    return text
```

**Example:**

```
Raw:   "„Åì„Çì„Å´ „Å°„ÅØ ABC 123 ‚Ä¶"
Clean: "„Åì„Çì„Å´„Å°„ÅØÔº°Ôº¢Ôº£ÔºëÔºíÔºì..."
```

#### 2. `run.py` (139 lines) - Background Watcher üëÅÔ∏è

**Two modes:**

```python
def run(read_from="clipboard", write_to="clipboard",
        force_cpu=False, delay_secs=0.1):
    mocr = MangaOcr(force_cpu=force_cpu)

    # MODE 1: Watch clipboard
    if read_from == "clipboard":
        from PIL import ImageGrab
        while True:
            img = ImageGrab.grabclipboard()
            if img_changed(img, old_img):
                text = mocr(img)
                pyperclip.copy(text)
            time.sleep(delay_secs)

    # MODE 2: Watch directory
    else:
        while True:
            for new_file in directory:
                text = mocr(new_file)
                append_to_file(text)
            time.sleep(delay_secs)
```

**Features:**

- Clipboard monitoring (Windows, Mac, Linux/Wayland)
- Directory watching for new images
- Output to clipboard or text file
- Configurable polling interval

#### 3. `__main__.py` (11 lines) - CLI Entry

```python
import fire
from manga_ocr.run import run

def main():
    fire.Fire(run)  # Auto-generate CLI

if __name__ == "__main__":
    main()
```

**CLI Usage:**

```bash
# Default: clipboard ‚Üí clipboard
manga_ocr

# Directory ‚Üí file
manga_ocr --read-from /images --write-to output.txt

# Force CPU
manga_ocr --force-cpu

# Custom model
manga_ocr --pretrained-model-name-or-path my-model
```

---

## Architecture & Data Flow

### OCR Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. INPUT: Image (file path or PIL Image)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. PREPROCESSING:                                           ‚îÇ
‚îÇ     - Load image with Pillow                                ‚îÇ
‚îÇ     - Convert: Grayscale ‚Üí RGB                              ‚îÇ
‚îÇ     - Resize to 384x384 (ViTImageProcessor)                 ‚îÇ
‚îÇ     - Normalize pixels to [-1, 1]                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. NEURAL NETWORK (VisionEncoderDecoderModel):             ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ     ‚îÇ  ViT Encoder     ‚îÇ  ‚Üí   ‚îÇ  BERT Decoder   ‚îÇ          ‚îÇ
‚îÇ     ‚îÇ  (Vision         ‚îÇ      ‚îÇ  (Text Gen)     ‚îÇ          ‚îÇ
‚îÇ     ‚îÇ  Transformer)    ‚îÇ      ‚îÇ                 ‚îÇ          ‚îÇ
‚îÇ     ‚îÇ  12 layers       ‚îÇ      ‚îÇ  12 layers      ‚îÇ          ‚îÇ
‚îÇ     ‚îÇ  768 hidden size ‚îÇ      ‚îÇ  768 hidden     ‚îÇ          ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ     Image Features (vectors) ‚Üí Japanese Text Tokens        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. TOKENIZATION:                                            ‚îÇ
‚îÇ     - Decode tokens ‚Üí Japanese characters                   ‚îÇ
‚îÇ     - Use vocab.txt (4000+ characters)                      ‚îÇ
‚îÇ     - Skip special tokens ([PAD], [CLS], etc.)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  5. POST-PROCESSING:                                         ‚îÇ
‚îÇ     - Remove all whitespace                                  ‚îÇ
‚îÇ     - Normalize ellipsis (‚Ä¶ ‚Üí ...)                          ‚îÇ
‚îÇ     - Convert half-width ‚Üí full-width (ABC ‚Üí Ôº°Ôº¢Ôº£)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  6. OUTPUT: Clean Japanese text string                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Model Architecture Details

**Base Models:**

- **Encoder:** `google/vit-base-patch16-224` (Vision Transformer)
  - Input: 384√ó384 RGB image
  - Patch size: 16√ó16 pixels
  - Hidden size: 768
  - Layers: 12 transformer blocks
  - Attention heads: 12

- **Decoder:** `cl-tohoku/bert-base-japanese-char-v2` (Japanese BERT)
  - Character-level tokenization
  - Hidden size: 768
  - Layers: 12 transformer blocks
  - Vocab size: 4000+ Japanese characters

**Fine-tuning:**

- Pre-trained on manga images
- Specialized for vertical text, speech bubbles, stylized fonts
- Recognizes hiragana, katakana, kanji

---

## Model Files Breakdown

### Directory Structure (Hugging Face Cache)

```
data/models/huggingface/hub/models--kha-white--manga-ocr-base/
‚îú‚îÄ‚îÄ blobs/                                    # Actual file content (content-addressed)
‚îÇ   ‚îú‚îÄ‚îÄ c63e0bb5...                          # pytorch_model.bin (424MB)
‚îÇ   ‚îú‚îÄ‚îÄ 218bc3b6...                          # model.safetensors (424MB) - DUPLICATE
‚îÇ   ‚îú‚îÄ‚îÄ deb78906...                          # config.json (76KB)
‚îÇ   ‚îú‚îÄ‚îÄ c2bd4e46...                          # vocab.txt (24KB)
‚îÇ   ‚îú‚îÄ‚îÄ a1f0a30b...                          # tokenizer_config.json (486B)
‚îÇ   ‚îú‚îÄ‚îÄ b7414e73...                          # preprocessor_config.json (228B)
‚îÇ   ‚îî‚îÄ‚îÄ e7b03750...                          # special_tokens_map.json (112B)
‚îú‚îÄ‚îÄ refs/
‚îÇ   ‚îî‚îÄ‚îÄ main                                  # Points to active snapshot
‚îî‚îÄ‚îÄ snapshots/
    ‚îî‚îÄ‚îÄ aa6573bd.../                         # Active snapshot (symlinks to blobs)
        ‚îú‚îÄ‚îÄ pytorch_model.bin ‚Üí ../../blobs/c63e0bb5...
        ‚îú‚îÄ‚îÄ config.json ‚Üí ../../blobs/deb78906...
        ‚îú‚îÄ‚îÄ vocab.txt ‚Üí ../../blobs/c2bd4e46...
        ‚îú‚îÄ‚îÄ tokenizer_config.json ‚Üí ../../blobs/a1f0a30b...
        ‚îú‚îÄ‚îÄ preprocessor_config.json ‚Üí ../../blobs/b7414e73...
        ‚îî‚îÄ‚îÄ special_tokens_map.json ‚Üí ../../blobs/e7b03750...
```

### File Details

#### 1. Neural Network Weights (424MB each) üî¥

**Two formats (DUPLICATE - only one needed):**

| File                | Size  | Format        | Used?       |
| ------------------- | ----- | ------------- | ----------- |
| `pytorch_model.bin` | 424MB | PyTorch (ZIP) | ‚úÖ Default  |
| `model.safetensors` | 424MB | SafeTensors   | ‚ö†Ô∏è Fallback |

**Content:** Millions of neural network parameters

- Encoder weights (ViT layers)
- Decoder weights (BERT layers)
- Attention matrices, embeddings, layer norms

**Loading:**

```python
self.model = MangaOcrModel.from_pretrained(...)
# ‚Üë Loads pytorch_model.bin by default
```

#### 2. Model Architecture Config (76KB)

**File:** `config.json`

```json
{
  "architectures": ["VisionEncoderDecoderModel"],
  "encoder": {
    "_name_or_path": "google/vit-base-patch16-224",
    "image_size": 384,
    "patch_size": 16,
    "num_hidden_layers": 12,
    "hidden_size": 768,
    "num_attention_heads": 12
  },
  "decoder": {
    "_name_or_path": "cl-tohoku/bert-base-japanese-char-v2",
    "num_hidden_layers": 12,
    "hidden_size": 768,
    "vocab_size": 4369
  }
}
```

**Purpose:** Defines model structure, layer sizes, architecture

#### 3. Vocabulary (24KB)

**File:** `vocab.txt` (4000+ lines)

```
[PAD]
[UNK]
[CLS]
[SEP]
[MASK]
!
"
#
...
„ÅÇ
„ÅÑ
„ÅÜ
...
„Ç¢
„Ç§
„Ç¶
...
‰∫ú
ÊÇ™
Âúß
...
```

**Purpose:** Maps token IDs ‚Üí Japanese characters

#### 4. Tokenizer Config (486 bytes)

**File:** `tokenizer_config.json`

```json
{
  "tokenizer_class": "BertJapaneseTokenizer",
  "do_lower_case": false,
  "word_tokenizer_type": "mecab",
  "subword_tokenizer_type": "character"
}
```

**Purpose:** Tokenizer behavior settings

#### 5. Image Preprocessor Config (228 bytes)

**File:** `preprocessor_config.json`

```json
{
  "do_normalize": true,
  "do_resize": true,
  "image_mean": [0.5, 0.5, 0.5],
  "image_std": [0.5, 0.5, 0.5],
  "size": { "height": 384, "width": 384 }
}
```

**Purpose:** Image preprocessing settings

#### 6. Special Tokens (112 bytes)

**File:** `special_tokens_map.json`

```json
{
  "cls_token": "[CLS]",
  "mask_token": "[MASK]",
  "pad_token": "[PAD]",
  "sep_token": "[SEP]",
  "unk_token": "[UNK]"
}
```

**Purpose:** Special token definitions

### Size Breakdown

```
Total: 848MB
‚îÇ
‚îú‚îÄ‚îÄ pytorch_model.bin        424MB (99.9%)  ‚Üê Neural weights (USED)
‚îú‚îÄ‚îÄ model.safetensors        424MB (duplicate) ‚Üê WASTED SPACE
‚îÇ
‚îú‚îÄ‚îÄ config.json               76KB (0.01%)
‚îú‚îÄ‚îÄ vocab.txt                 24KB (0.003%)
‚îú‚îÄ‚îÄ tokenizer_config.json    486B (tiny)
‚îú‚îÄ‚îÄ preprocessor_config.json 228B (tiny)
‚îî‚îÄ‚îÄ special_tokens_map.json  112B (tiny)
```

**Memory Usage During Inference:**

- Model weights in RAM: ~424MB
- PyTorch runtime overhead: ~500MB
- **Total RAM: ~900MB-1GB**

---

## Docker Image Size Analysis

### Problem: 8GB Docker Image

**Current Dockerfile.base:**

```dockerfile
FROM python:3.11-slim          # ~150MB
RUN pip install manga-ocr      # Downloads CUDA PyTorch (~3-4GB)
```

### Size Breakdown

```
Base Image (python:3.11-slim):        ~150 MB
PyTorch (CUDA version):             ~3,000 MB  ‚Üê MAIN PROBLEM
Transformers:                         ~500 MB
manga-ocr + dependencies:             ~100 MB
Models (in volumes, not image):       ~400 MB
CUDA dependencies:                  ~3,802 MB  ‚Üê CUDA drivers/toolkit
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total:                               ~8,002 MB (8.02 GB)
```

### Why CUDA PyTorch is Installed

When `pip install manga-ocr` runs, it installs `torch>=1.0` which defaults to:

- **CUDA-enabled version** (~3-4GB)
- Includes NVIDIA CUDA toolkit
- Includes cuDNN libraries
- GPU driver dependencies

**You're running on CPU, so this is completely wasted!**

---

## Optimization Recommendations

### 1. Use CPU-only PyTorch (Recommended) ‚≠ê

**Reduces image: 8GB ‚Üí 1.5-2GB (75% reduction)**

**Modified Dockerfile.base:**

```dockerfile
FROM python:3.11-slim

ENV DEBIAN_FRONTEND=noninteractive

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Install CPU-only PyTorch FIRST (prevents CUDA download)
RUN pip install --no-cache-dir \
    torch==2.1.2+cpu \
    torchvision==0.16.2+cpu \
    --index-url https://download.pytorch.org/whl/cpu

# Then install manga-ocr (will use already-installed torch)
RUN pip install --no-cache-dir manga-ocr==0.1.14

# Verify installation
RUN python -c "from manga_ocr import MangaOcr; print('manga-ocr installed successfully!')"

WORKDIR /app
```

**Why this works:**

- Installing torch BEFORE manga-ocr prevents CUDA download
- Uses PyTorch's CPU-specific wheel repository
- ~900MB instead of ~3-4GB

### 2. Multi-stage Build (Advanced)

**Reduces image: 8GB ‚Üí 1.2-1.5GB (80% reduction)**

```dockerfile
# Stage 1: Builder
FROM python:3.11-slim AS builder

RUN apt-get update && apt-get install -y \
    git wget gcc g++ \
    && rm -rf /var/lib/apt/lists/*

# Create virtual environment
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Install CPU-only PyTorch
RUN pip install --no-cache-dir \
    torch==2.1.2+cpu \
    torchvision==0.16.2+cpu \
    --index-url https://download.pytorch.org/whl/cpu

# Install manga-ocr
RUN pip install --no-cache-dir manga-ocr==0.1.14

# Stage 2: Runtime (smaller!)
FROM python:3.11-slim

# Copy only virtual environment
COPY --from=builder /opt/venv /opt/venv

# Install minimal runtime dependencies
RUN apt-get update && apt-get install -y \
    --no-install-recommends \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/*

ENV PATH="/opt/venv/bin:$PATH"

RUN python -c "from manga_ocr import MangaOcr; print('manga-ocr installed successfully!')"

WORKDIR /app
```

**Benefits:**

- Build tools (gcc, g++) only in stage 1
- Final image has only runtime dependencies
- Smaller base image

### 3. Remove Duplicate Model Weights

**Saves 424MB from model cache:**

```bash
# Keep only one format (pytorch_model.bin is default)
cd data/models/huggingface/hub/models--kha-white--manga-ocr-base/
rm snapshots/*/model.safetensors
rm blobs/218bc3b6f1bce855a8d6eb86150de1f0407115627e68729ab36a6e4f33480ad9
```

**Result:** 848MB ‚Üí 424MB model cache

### 4. Alpine Linux (Most Aggressive)

**Reduces: 8GB ‚Üí 800MB-1.2GB (85% reduction)**

```dockerfile
FROM python:3.11-alpine

# Install runtime dependencies
RUN apk add --no-cache \
    libstdc++ \
    libgomp \
    git \
    wget

# Install build dependencies (will be removed)
RUN apk add --no-cache --virtual .build-deps \
    gcc \
    g++ \
    musl-dev \
    linux-headers

# Install CPU-only PyTorch
RUN pip install --no-cache-dir \
    torch==2.1.2+cpu \
    torchvision==0.16.2+cpu \
    --index-url https://download.pytorch.org/whl/cpu

# Install manga-ocr
RUN pip install --no-cache-dir manga-ocr==0.1.14

# Remove build dependencies
RUN apk del .build-deps

RUN python -c "from manga_ocr import MangaOcr; print('manga-ocr installed successfully!')"

WORKDIR /app
```

‚ö†Ô∏è **Warning:** Alpine can have compatibility issues with some Python packages

### 5. Expected Results

| Strategy             | Image Size   | Reduction   | Complexity  |
| -------------------- | ------------ | ----------- | ----------- |
| **Current (CUDA)**   | **8.0 GB**   | ‚Äî           | ‚Äî           |
| **CPU-only PyTorch** | **1.5-2 GB** | **75%**     | ‚≠ê Easy     |
| Multi-stage          | 1.2-1.5 GB   | 80%         | ‚≠ê‚≠ê Medium |
| Remove duplicates    | -424 MB      | Model cache | ‚≠ê Easy     |
| Alpine               | 800MB-1.2GB  | 85%         | ‚≠ê‚≠ê‚≠ê Hard |

**Recommended:** Start with CPU-only PyTorch (easiest, biggest impact)

### 6. Rebuild Instructions

```bash
# 1. Update Dockerfile.base with CPU-only torch
# (see above)

# 2. Rebuild base image
./build-base.sh

# 3. Rebuild app image
docker-compose build

# 4. Verify size
docker images | grep manga-reader
# Should show ~1.5-2GB instead of 8GB
```

---

## Summary

### Key Insights

1. **manga-ocr is simple** - Only 217 lines wrapping Hugging Face transformers
2. **Dependencies are heavy** - PyTorch (~3-4GB CUDA) is the main culprit
3. **Model is moderate** - 424MB neural weights + 100KB configs
4. **Optimization is easy** - Just use CPU-only PyTorch!

### Quick Wins

‚úÖ **Use CPU-only PyTorch** ‚Üí Save 2.5GB (75% reduction)
‚úÖ **Remove duplicate model** ‚Üí Save 424MB (model cache)
‚úÖ **Multi-stage build** ‚Üí Save additional 300-500MB

### Architecture Understanding

```
manga-ocr = Thin wrapper
    ‚Üì
Hugging Face Transformers = Model framework
    ‚Üì
PyTorch = Deep learning engine
    ‚Üì
Pre-trained ViT + BERT = The actual intelligence
```

**The "magic" isn't in manga-ocr's code - it's in the pre-trained model!**

---

## References

- **Repository:** https://github.com/kha-white/manga-ocr
- **Model:** https://huggingface.co/kha-white/manga-ocr-base
- **PyTorch CPU wheels:** https://download.pytorch.org/whl/cpu
- **Transformers docs:** https://huggingface.co/docs/transformers

---

**Last Updated:** 2026-02-03
**Analyzed Version:** manga-ocr 0.1.14
